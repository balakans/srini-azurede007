{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "220a856e-eeb5-4afd-8355-a1b4bdb1c27d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.appName(\"Spark Read\").getOrCreate()\n",
    "#uri -> uniform resource identifier\n",
    "#ex: dbfs:/, hive:/,hdfs:/, gs:/, adls:/, s3:/\n",
    "df = spark.read.csv(\"dbfs:/Volumes/workspace/default/tblcustomer/custs\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a145a7c7-447f-4a31-beba-90a02bf7154c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Volumes/inceptez_catalog/inputdb/customerdata/countries_population.csv\")\n",
    "\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7cdbf2-6f01-45a7-9dd0-9f3afad99be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Volumes/inceptez_catalog/inputdb/customerdata/countries_population.csv\",header=True)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009a120e-864d-4a4d-9117-147d3664dbe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8319201-e6bd-473f-bc0d-ea42c8ac4086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Volumes/inceptez_catalog/inputdb/customerdata/countries_population.csv\", header=True,inferSchema=True,sep=\",\")\n",
    "\n",
    "# To display the structure of the dataframe\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b21f30bc-c061-473b-af9f-22698820abcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.options(header=True).options(inferSchema=True).csv(\"/Volumes/inceptez_catalog/inputdb/customerdata/countries_population.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab7357e4-dc93-46c3-afa3-b8d1268d855c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generic way of read and load data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a490451e-408b-4122-8767-51825a0c3ae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").options(header=True).options(inferSchema=True).load(\"/Volumes/inceptez_catalog/inputdb/customerdata/countries_population.csv\")\n",
    "cnt = df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "058e11df-a6de-4acf-8fc4-162b237b0f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading data from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8469249-bcb4-4d50-b236-edd4d48c9ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.option(\"header\", True).csv(\"dbfs:/Volumes/inceptez_catalog/inputdb/customerdata/*.csv\")\n",
    "#or\n",
    "df = spark.read.csv([\"dbfs:/Volumes/inceptez_catalog/inputdb/customerdatacountries_population.csv\", \n",
    "                     \"dbfs:/Volumes/inceptez_catalog/inputdb/customerdatacountries_population1.csv\"], \n",
    "                    header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14f5f198-1d9f-4803-8bb8-35690e072b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").options(header=True).options(inferSchema=True).load(\"dbfs:/Volumes/inceptez_catalog/inputdb/customerdata/*.csv\")\n",
    "\n",
    "df = spark.read.format(\"csv\").options(header=True).options(inferSchema=True).load([\"dbfs:/Volumes/inceptez_catalog/inputdb/customerdatacountries_population.csv\", \n",
    "                     \"dbfs:/Volumes/inceptez_catalog/inputdb/customerdatacountries_population1.csv\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eccae843-0a73-4526-ae12-2349f4a64533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Provide schema with SQL String\n",
    "\n",
    "[PySpark SQL Datatypes](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b708f7cf-b676-489c-a6b0-f9f6504795a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strschema = \"country_id integar, name string, nationality string, country_code string, iso_alpha2 string, capital string, population int, area_km2 double, region_id int, sub_region_id int\"\n",
    "\n",
    "df = spark.read.format(\"csv\").options(header=True).schema(strschema).load(\"/Volumes/inceptez_catalog/inputdb/customerdata/countries_population.csv\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5064cbd2-4594-4ad9-915f-d9ecabd97ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define schema programmatically\n",
    "[Data Types](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6300d76e-f7b7-418a-8515-9cdff10fa90e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "countryschema = StructType([\n",
    "    StructField('COUNTRY_ID', IntegerType(), True), \n",
    "    StructField('NAME', StringType(), True), \n",
    "    StructField('NATIONALITY', StringType(), True), \n",
    "    StructField('COUNTRY_CODE', StringType(), True), \n",
    "    StructField('ISO_ALPHA2', StringType(), True), \n",
    "    StructField('CAPITAL', StringType(), True), \n",
    "    StructField('POPULATION', IntegerType(), True), \n",
    "    StructField('AREA_KM2', DoubleType(), True), \n",
    "    StructField('REGION_ID', IntegerType(), True), \n",
    "    StructField('SUB_REGION_ID', IntegerType(), True)])\n",
    "\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .schema(countryschema)\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"delimiter\", \",\")\n",
    "      .load(\"dbfs:/Volumes/workspace/default/countries/countries_population.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a02cb46-c403-41a1-aec0-1f2613a64da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To read 10 records\n",
    "df = spark.read.format(\"csv\").options(header=True).options(inferSchema=True).load(\"/Volumes/inceptez_catalog/inputdb/customerdata/countries_population.csv\").limit(10)\n",
    "\n",
    "df.schema\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab01-Readops",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
